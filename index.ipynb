{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab on initialization and optimization, let's look at a slightly different type of neural network. This time, we will not perform a classification task as we've done before.  Instead, we'll look at a regression problem.\n",
    "\n",
    "We can just as well use deep learning networks for regression as for a classification problem. However, note that getting regression to work with neural networks is a harder problem because the output is unbounded ($\\hat y$ can technically range from $-\\infty$ to $+\\infty$, and the models are especially prone to **_exploding gradients_**. This issue makes a regression exercise the perfect learning case!\n",
    "\n",
    "Run the cell below to import everything we'll need for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras import initializers\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we'll be working with is data related to facebook posts published during the year of 2014 on the Facebook's page of a renowned cosmetics brand.  It includes 7 features known prior to post publication, and 12 features for evaluating the post impact. What we want to do is make a predictor for the number of \"likes\" for a post, taking into account the 7 features prior to posting.\n",
    "\n",
    "First, let's import the data set and delete any rows with missing data.  \n",
    "\n",
    "The dataset is contained with the file `dataset_Facebook.csv`. In the cell below, use pandas to read in the data from this file. Because of the way the data is structure, make sure you also set the `sep` parameter to `\";\"`, and the `header` parameter to `0`. \n",
    "\n",
    "Then, use the DataFrame's built-in `.dropna()` function to remove any rows with missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv(\"dataset_Facebook.csv\", sep = \";\", header=0)\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the shape of our data to ensure that everything looks correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(495, 19)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data) #Expected Output: (495, 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's inspect the `.head()` of the DataFrame to get a feel for what our dataset looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page total likes</th>\n",
       "      <th>Type</th>\n",
       "      <th>Category</th>\n",
       "      <th>Post Month</th>\n",
       "      <th>Post Weekday</th>\n",
       "      <th>Post Hour</th>\n",
       "      <th>Paid</th>\n",
       "      <th>Lifetime Post Total Reach</th>\n",
       "      <th>Lifetime Post Total Impressions</th>\n",
       "      <th>Lifetime Engaged Users</th>\n",
       "      <th>Lifetime Post Consumers</th>\n",
       "      <th>Lifetime Post Consumptions</th>\n",
       "      <th>Lifetime Post Impressions by people who have liked your Page</th>\n",
       "      <th>Lifetime Post reach by people who like your Page</th>\n",
       "      <th>Lifetime People who have liked your Page and engaged with your post</th>\n",
       "      <th>comment</th>\n",
       "      <th>like</th>\n",
       "      <th>share</th>\n",
       "      <th>Total Interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2752</td>\n",
       "      <td>5091</td>\n",
       "      <td>178</td>\n",
       "      <td>109</td>\n",
       "      <td>159</td>\n",
       "      <td>3078</td>\n",
       "      <td>1640</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>79.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139441</td>\n",
       "      <td>Status</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10460</td>\n",
       "      <td>19057</td>\n",
       "      <td>1457</td>\n",
       "      <td>1361</td>\n",
       "      <td>1674</td>\n",
       "      <td>11710</td>\n",
       "      <td>6112</td>\n",
       "      <td>1108</td>\n",
       "      <td>5</td>\n",
       "      <td>130.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2413</td>\n",
       "      <td>4373</td>\n",
       "      <td>177</td>\n",
       "      <td>113</td>\n",
       "      <td>154</td>\n",
       "      <td>2812</td>\n",
       "      <td>1503</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50128</td>\n",
       "      <td>87991</td>\n",
       "      <td>2211</td>\n",
       "      <td>790</td>\n",
       "      <td>1119</td>\n",
       "      <td>61027</td>\n",
       "      <td>32048</td>\n",
       "      <td>1386</td>\n",
       "      <td>58</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>1777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7244</td>\n",
       "      <td>13594</td>\n",
       "      <td>671</td>\n",
       "      <td>410</td>\n",
       "      <td>580</td>\n",
       "      <td>6228</td>\n",
       "      <td>3200</td>\n",
       "      <td>396</td>\n",
       "      <td>19</td>\n",
       "      <td>325.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Page total likes    Type  Category  Post Month  Post Weekday  Post Hour  \\\n",
       "0            139441   Photo         2          12             4          3   \n",
       "1            139441  Status         2          12             3         10   \n",
       "2            139441   Photo         3          12             3          3   \n",
       "3            139441   Photo         2          12             2         10   \n",
       "4            139441   Photo         2          12             2          3   \n",
       "\n",
       "   Paid  Lifetime Post Total Reach  Lifetime Post Total Impressions  \\\n",
       "0   0.0                       2752                             5091   \n",
       "1   0.0                      10460                            19057   \n",
       "2   0.0                       2413                             4373   \n",
       "3   1.0                      50128                            87991   \n",
       "4   0.0                       7244                            13594   \n",
       "\n",
       "   Lifetime Engaged Users  Lifetime Post Consumers  \\\n",
       "0                     178                      109   \n",
       "1                    1457                     1361   \n",
       "2                     177                      113   \n",
       "3                    2211                      790   \n",
       "4                     671                      410   \n",
       "\n",
       "   Lifetime Post Consumptions  \\\n",
       "0                         159   \n",
       "1                        1674   \n",
       "2                         154   \n",
       "3                        1119   \n",
       "4                         580   \n",
       "\n",
       "   Lifetime Post Impressions by people who have liked your Page  \\\n",
       "0                                               3078              \n",
       "1                                              11710              \n",
       "2                                               2812              \n",
       "3                                              61027              \n",
       "4                                               6228              \n",
       "\n",
       "   Lifetime Post reach by people who like your Page  \\\n",
       "0                                              1640   \n",
       "1                                              6112   \n",
       "2                                              1503   \n",
       "3                                             32048   \n",
       "4                                              3200   \n",
       "\n",
       "   Lifetime People who have liked your Page and engaged with your post  \\\n",
       "0                                                119                     \n",
       "1                                               1108                     \n",
       "2                                                132                     \n",
       "3                                               1386                     \n",
       "4                                                396                     \n",
       "\n",
       "   comment    like  share  Total Interactions  \n",
       "0        4    79.0   17.0                 100  \n",
       "1        5   130.0   29.0                 164  \n",
       "2        0    66.0   14.0                  80  \n",
       "3       58  1572.0  147.0                1777  \n",
       "4       19   325.0   49.0                 393  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Normalize the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big part of Deep Learning is cleaning the data and getting into a shape usable by a neural network.  Let's get some additional practice with this.\n",
    "\n",
    "\n",
    "Take a look at our input data. We'll use the 7 first columns as our predictors. We'll do the following two things:\n",
    "- Normalize the continuous variables --> you can do this using `np.mean()` and `np.std()`\n",
    "- make dummy variables of the categorical variables (you can do this by using `pd.get_dummies`)\n",
    "\n",
    "We only count \"Category\" and \"Type\" as categorical variables. Note that you can argue that \"Post month\", \"Post Weekday\" and \"Post Hour\" can also be considered categories, but we'll just treat them as being continuous for now.\n",
    "\n",
    "In the cell below, convert the data as needed by normalizing or converting to dummy variables, and then concatenate it all back into a single DataFrame once you've finished.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = data[\"Page total likes\"]\n",
    "X1 = data[\"Type\"]\n",
    "X2 = data[\"Category\"]\n",
    "X3 = data[\"Post Month\"]\n",
    "X4 = data[\"Post Weekday\"]\n",
    "X5 = data[\"Post Hour\"]\n",
    "X6 = data[\"Paid\"]\n",
    "\n",
    "## standardize/categorize\n",
    "X0= (X0-np.mean(X0))/(np.std(X0))\n",
    "dummy_X1= pd.get_dummies(X1)\n",
    "dummy_X2= pd.get_dummies(X2)\n",
    "X3= (X3-np.mean(X3))/(np.std(X3))\n",
    "X4= (X4-np.mean(X4))/(np.std(X4))\n",
    "X5= (X5-np.mean(X5))/(np.std(X5))\n",
    "\n",
    "# Add them all back into a single DataFrame\n",
    "X = pd.concat([X0, dummy_X1, dummy_X2, X3, X4, X5, X6], axis=1)\n",
    "\n",
    "# Store our labels in a separate variable\n",
    "Y = data[\"like\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: you get the same result for standardization if you use StandardScaler from sklearn.preprocessing\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc = StandardScaler()\n",
    "#X0 = sc.fit_transform(X0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is fairly small. Let's just split the data up in a training set and a validation set!\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Split the data into training and testing sets by passing `X` and `Y` into `train_test_split`.  Set a `test_size` of `0.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape to make sure everything worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape # Expected Output: (99, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(396, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape # Expected Output: (396, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network for Regression\n",
    "\n",
    "Now, we'll build a neural network to predict the number of likes we think a post will receive.  \n",
    "\n",
    "In the cell below, create a model with the following specifications:\n",
    "\n",
    "* 1 Hidden Layer with 8 neurons.  In this layer, also set `input_dim` to `12`, and `activation` to `\"relu\"`.\n",
    "* An output layer with 1 neuron.  For this neuron, set the activation to `linear`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(Dense(1, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to compile the model, with the following hyperparameters:\n",
    "\n",
    "* `optimizer='sgd'`\n",
    "* `loss='mse'`\n",
    "* `metrics=['mse']`\n",
    "\n",
    "Note that since our model is training for a regression task, not a classification task, we'll need to use a loss metric that corresponds with regression tasks--Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's train the model.  Call `model.fit()`. In addition to to the training data and labels, also set:\n",
    "\n",
    "* `batch_size=32`\n",
    "* `epochs=100`\n",
    "* `verbose=1`\n",
    "* `validation_data=(X_val, y_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "396/396 [==============================] - 0s 141us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 2/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 3/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 4/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 5/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 6/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 7/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 8/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 9/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 10/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 11/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 12/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 13/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 14/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 15/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 16/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 17/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 18/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 19/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 20/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 21/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 22/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 23/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 24/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 25/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 26/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 27/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 28/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 29/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 30/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 31/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 32/100\n",
      "396/396 [==============================] - 0s 45us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 33/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 34/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 35/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 36/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 37/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 38/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 39/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 40/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 41/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 42/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 43/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 44/100\n",
      "396/396 [==============================] - 0s 70us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 45/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 46/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 47/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 48/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 49/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 50/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 51/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 52/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 53/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 45us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 55/100\n",
      "396/396 [==============================] - 0s 45us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 56/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 57/100\n",
      "396/396 [==============================] - 0s 44us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 58/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 59/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 60/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 61/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 62/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 63/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 64/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 65/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 66/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 67/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 68/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 69/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 70/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 71/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 72/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 73/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 74/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 75/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 76/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 77/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 78/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 79/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 80/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 81/100\n",
      "396/396 [==============================] - 0s 42us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 82/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 83/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 84/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 85/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 86/100\n",
      "396/396 [==============================] - 0s 44us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 87/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 88/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 89/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 90/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 91/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 92/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 93/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 94/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 95/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 96/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 97/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 98/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 99/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 100/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you see what happend? all the values for training and validation loss are \"nan\". There could be several reasons for that, but as we already mentioned there is likely a vanishing or exploding gradient problem.  This means that the values got so large or so small that they no longer fit in memory.   R\n",
    "\n",
    "Recall that we normalized out inputs. But how about the outputs? Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212     36.0\n",
       "107    193.0\n",
       "411     75.0\n",
       "71     449.0\n",
       "473    136.0\n",
       "Name: like, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, indeed. We didn't normalize them and we should, as they take pretty high values. Let\n",
    "s rerun the model but make sure that the output is normalized as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Normalizing the output\n",
    "\n",
    "In the cell below, we've included all the normalization code that we wrote up top, but this time, we've added a line to normalize the data in `Y`, as well. This should help alot!\n",
    "\n",
    "Run the cell below to normalize our data and our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = data[\"Page total likes\"]\n",
    "X1 = data[\"Type\"]\n",
    "X2 = data[\"Category\"]\n",
    "X3 = data[\"Post Month\"]\n",
    "X4 = data[\"Post Weekday\"]\n",
    "X5 = data[\"Post Hour\"]\n",
    "X6 = data[\"Paid\"]\n",
    "\n",
    "## standardize/categorize\n",
    "X0= (X0-np.mean(X0))/(np.std(X0))\n",
    "dummy_X1= pd.get_dummies(X1)\n",
    "dummy_X2= pd.get_dummies(X2)\n",
    "X3= (X3-np.mean(X3))/(np.std(X3))\n",
    "X4= (X4-np.mean(X4))/(np.std(X4))\n",
    "X5= (X5-np.mean(X5))/(np.std(X5))\n",
    "\n",
    "X = pd.concat([X0, dummy_X1, dummy_X2, X3, X4, X5, X6], axis=1)\n",
    "\n",
    "Y = (data[\"like\"]-np.mean(data[\"like\"]))/(np.std(data[\"like\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split our data into appropriate training and testing sets again.  Split the data, just like we did before.  Use the same `test_size` as we did last time, too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's reinitialize our model and build it from scratch again.  \n",
    "\n",
    "**_NOTE:_**  If we don't reinitialize our model, our training would start with the weight values we ended with during the last training session.  In order to start fresh, we need to declare a new `Sequential()` object.  \n",
    "\n",
    "Build the model with the exact same architecture and hyperparameters as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(Dense(1, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compile the model with the same parameters we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, fit the model using the same parameters we did before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "396/396 [==============================] - 0s 157us/step - loss: 1.7564 - mean_squared_error: 1.7564 - val_loss: 0.3350 - val_mean_squared_error: 0.3350\n",
      "Epoch 2/100\n",
      "396/396 [==============================] - 0s 44us/step - loss: 1.3417 - mean_squared_error: 1.3417 - val_loss: 0.3003 - val_mean_squared_error: 0.3003\n",
      "Epoch 3/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: 1.2700 - mean_squared_error: 1.2700 - val_loss: 0.3007 - val_mean_squared_error: 0.3007\n",
      "Epoch 4/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.2396 - mean_squared_error: 1.2396 - val_loss: 0.2924 - val_mean_squared_error: 0.2924\n",
      "Epoch 5/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.2187 - mean_squared_error: 1.2187 - val_loss: 0.2871 - val_mean_squared_error: 0.2871\n",
      "Epoch 6/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: 1.2036 - mean_squared_error: 1.2036 - val_loss: 0.2840 - val_mean_squared_error: 0.2840\n",
      "Epoch 7/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: 1.1944 - mean_squared_error: 1.1944 - val_loss: 0.2734 - val_mean_squared_error: 0.2734\n",
      "Epoch 8/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.1836 - mean_squared_error: 1.1836 - val_loss: 0.2706 - val_mean_squared_error: 0.2706\n",
      "Epoch 9/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 1.1727 - mean_squared_error: 1.1727 - val_loss: 0.2823 - val_mean_squared_error: 0.2823\n",
      "Epoch 10/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: 1.1647 - mean_squared_error: 1.1647 - val_loss: 0.2893 - val_mean_squared_error: 0.2893\n",
      "Epoch 11/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.1561 - mean_squared_error: 1.1561 - val_loss: 0.2701 - val_mean_squared_error: 0.2701\n",
      "Epoch 12/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 1.1517 - mean_squared_error: 1.1517 - val_loss: 0.2773 - val_mean_squared_error: 0.2773\n",
      "Epoch 13/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.1462 - mean_squared_error: 1.1462 - val_loss: 0.2630 - val_mean_squared_error: 0.2630\n",
      "Epoch 14/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.1422 - mean_squared_error: 1.1422 - val_loss: 0.2688 - val_mean_squared_error: 0.2688\n",
      "Epoch 15/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: 1.1383 - mean_squared_error: 1.1383 - val_loss: 0.2531 - val_mean_squared_error: 0.2531\n",
      "Epoch 16/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 1.1368 - mean_squared_error: 1.1368 - val_loss: 0.2582 - val_mean_squared_error: 0.2582\n",
      "Epoch 17/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.1345 - mean_squared_error: 1.1345 - val_loss: 0.2619 - val_mean_squared_error: 0.2619\n",
      "Epoch 18/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.1330 - mean_squared_error: 1.1330 - val_loss: 0.2666 - val_mean_squared_error: 0.2666\n",
      "Epoch 19/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.1296 - mean_squared_error: 1.1296 - val_loss: 0.2803 - val_mean_squared_error: 0.2803\n",
      "Epoch 20/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.1279 - mean_squared_error: 1.1279 - val_loss: 0.2630 - val_mean_squared_error: 0.2630\n",
      "Epoch 21/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.1266 - mean_squared_error: 1.1266 - val_loss: 0.2556 - val_mean_squared_error: 0.2556\n",
      "Epoch 22/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.1236 - mean_squared_error: 1.1236 - val_loss: 0.2602 - val_mean_squared_error: 0.2602\n",
      "Epoch 23/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: 1.1199 - mean_squared_error: 1.1199 - val_loss: 0.2481 - val_mean_squared_error: 0.2481\n",
      "Epoch 24/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1208 - mean_squared_error: 1.1208 - val_loss: 0.2550 - val_mean_squared_error: 0.2550\n",
      "Epoch 25/100\n",
      "396/396 [==============================] - 0s 64us/step - loss: 1.1187 - mean_squared_error: 1.1187 - val_loss: 0.2657 - val_mean_squared_error: 0.2657\n",
      "Epoch 26/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.1155 - mean_squared_error: 1.1155 - val_loss: 0.2653 - val_mean_squared_error: 0.2653\n",
      "Epoch 27/100\n",
      "396/396 [==============================] - 0s 59us/step - loss: 1.1133 - mean_squared_error: 1.1133 - val_loss: 0.2827 - val_mean_squared_error: 0.2827\n",
      "Epoch 28/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1109 - mean_squared_error: 1.1109 - val_loss: 0.2574 - val_mean_squared_error: 0.2574\n",
      "Epoch 29/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.1115 - mean_squared_error: 1.1115 - val_loss: 0.2590 - val_mean_squared_error: 0.2590\n",
      "Epoch 30/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.1099 - mean_squared_error: 1.1099 - val_loss: 0.2621 - val_mean_squared_error: 0.2621\n",
      "Epoch 31/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1092 - mean_squared_error: 1.1092 - val_loss: 0.2676 - val_mean_squared_error: 0.2676\n",
      "Epoch 32/100\n",
      "396/396 [==============================] - 0s 62us/step - loss: 1.1082 - mean_squared_error: 1.1082 - val_loss: 0.2687 - val_mean_squared_error: 0.2687\n",
      "Epoch 33/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.1074 - mean_squared_error: 1.1074 - val_loss: 0.2609 - val_mean_squared_error: 0.2609\n",
      "Epoch 34/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.1064 - mean_squared_error: 1.1064 - val_loss: 0.2577 - val_mean_squared_error: 0.2577\n",
      "Epoch 35/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.1081 - mean_squared_error: 1.1081 - val_loss: 0.2548 - val_mean_squared_error: 0.2548\n",
      "Epoch 36/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.1048 - mean_squared_error: 1.1048 - val_loss: 0.2609 - val_mean_squared_error: 0.2609\n",
      "Epoch 37/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.1018 - mean_squared_error: 1.1018 - val_loss: 0.2697 - val_mean_squared_error: 0.2697\n",
      "Epoch 38/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1022 - mean_squared_error: 1.1022 - val_loss: 0.2647 - val_mean_squared_error: 0.2647\n",
      "Epoch 39/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.1010 - mean_squared_error: 1.1010 - val_loss: 0.2692 - val_mean_squared_error: 0.2692\n",
      "Epoch 40/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.0985 - mean_squared_error: 1.0985 - val_loss: 0.2885 - val_mean_squared_error: 0.2885\n",
      "Epoch 41/100\n",
      "396/396 [==============================] - 0s 57us/step - loss: 1.0990 - mean_squared_error: 1.0990 - val_loss: 0.2637 - val_mean_squared_error: 0.2637\n",
      "Epoch 42/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: 1.0977 - mean_squared_error: 1.0977 - val_loss: 0.2552 - val_mean_squared_error: 0.2552\n",
      "Epoch 43/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.0979 - mean_squared_error: 1.0979 - val_loss: 0.2570 - val_mean_squared_error: 0.2570\n",
      "Epoch 44/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.0963 - mean_squared_error: 1.0963 - val_loss: 0.2592 - val_mean_squared_error: 0.2592\n",
      "Epoch 45/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.0930 - mean_squared_error: 1.0930 - val_loss: 0.2602 - val_mean_squared_error: 0.2602\n",
      "Epoch 46/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0924 - mean_squared_error: 1.0924 - val_loss: 0.2623 - val_mean_squared_error: 0.2623\n",
      "Epoch 47/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: 1.0919 - mean_squared_error: 1.0919 - val_loss: 0.2625 - val_mean_squared_error: 0.2625\n",
      "Epoch 48/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: 1.0915 - mean_squared_error: 1.0915 - val_loss: 0.2561 - val_mean_squared_error: 0.2561\n",
      "Epoch 49/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: 1.0919 - mean_squared_error: 1.0919 - val_loss: 0.2517 - val_mean_squared_error: 0.2517\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 46us/step - loss: 1.0905 - mean_squared_error: 1.0905 - val_loss: 0.2573 - val_mean_squared_error: 0.2573\n",
      "Epoch 51/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0907 - mean_squared_error: 1.0907 - val_loss: 0.2604 - val_mean_squared_error: 0.2604\n",
      "Epoch 52/100\n",
      "396/396 [==============================] - 0s 45us/step - loss: 1.0878 - mean_squared_error: 1.0878 - val_loss: 0.2538 - val_mean_squared_error: 0.2538\n",
      "Epoch 53/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: 1.0877 - mean_squared_error: 1.0877 - val_loss: 0.2496 - val_mean_squared_error: 0.2496\n",
      "Epoch 54/100\n",
      "396/396 [==============================] - 0s 45us/step - loss: 1.0882 - mean_squared_error: 1.0882 - val_loss: 0.2553 - val_mean_squared_error: 0.2553\n",
      "Epoch 55/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0862 - mean_squared_error: 1.0862 - val_loss: 0.2617 - val_mean_squared_error: 0.2617\n",
      "Epoch 56/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0858 - mean_squared_error: 1.0858 - val_loss: 0.2833 - val_mean_squared_error: 0.2833\n",
      "Epoch 57/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.0832 - mean_squared_error: 1.0832 - val_loss: 0.2800 - val_mean_squared_error: 0.2800\n",
      "Epoch 58/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.0825 - mean_squared_error: 1.0825 - val_loss: 0.2727 - val_mean_squared_error: 0.2727\n",
      "Epoch 59/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.0826 - mean_squared_error: 1.0826 - val_loss: 0.2894 - val_mean_squared_error: 0.2894\n",
      "Epoch 60/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: 1.0834 - mean_squared_error: 1.0834 - val_loss: 0.2752 - val_mean_squared_error: 0.2752\n",
      "Epoch 61/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0810 - mean_squared_error: 1.0810 - val_loss: 0.2650 - val_mean_squared_error: 0.2650\n",
      "Epoch 62/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.0818 - mean_squared_error: 1.0818 - val_loss: 0.2640 - val_mean_squared_error: 0.2640\n",
      "Epoch 63/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.0801 - mean_squared_error: 1.0801 - val_loss: 0.2709 - val_mean_squared_error: 0.2709\n",
      "Epoch 64/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.0800 - mean_squared_error: 1.0800 - val_loss: 0.2661 - val_mean_squared_error: 0.2661\n",
      "Epoch 65/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: 1.0781 - mean_squared_error: 1.0781 - val_loss: 0.2737 - val_mean_squared_error: 0.2737\n",
      "Epoch 66/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0793 - mean_squared_error: 1.0793 - val_loss: 0.2629 - val_mean_squared_error: 0.2629\n",
      "Epoch 67/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.0771 - mean_squared_error: 1.0771 - val_loss: 0.2593 - val_mean_squared_error: 0.2593\n",
      "Epoch 68/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.0777 - mean_squared_error: 1.0777 - val_loss: 0.2864 - val_mean_squared_error: 0.2864\n",
      "Epoch 69/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.0773 - mean_squared_error: 1.0773 - val_loss: 0.2755 - val_mean_squared_error: 0.2755\n",
      "Epoch 70/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: 1.0745 - mean_squared_error: 1.0745 - val_loss: 0.2791 - val_mean_squared_error: 0.2791\n",
      "Epoch 71/100\n",
      "396/396 [==============================] - 0s 52us/step - loss: 1.0751 - mean_squared_error: 1.0751 - val_loss: 0.2670 - val_mean_squared_error: 0.2670\n",
      "Epoch 72/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0738 - mean_squared_error: 1.0738 - val_loss: 0.2730 - val_mean_squared_error: 0.2730\n",
      "Epoch 73/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0719 - mean_squared_error: 1.0719 - val_loss: 0.2752 - val_mean_squared_error: 0.2752\n",
      "Epoch 74/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0731 - mean_squared_error: 1.0731 - val_loss: 0.3143 - val_mean_squared_error: 0.3143\n",
      "Epoch 75/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0758 - mean_squared_error: 1.0758 - val_loss: 0.2964 - val_mean_squared_error: 0.2964\n",
      "Epoch 76/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.0750 - mean_squared_error: 1.0750 - val_loss: 0.2819 - val_mean_squared_error: 0.2819\n",
      "Epoch 77/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.0722 - mean_squared_error: 1.0722 - val_loss: 0.2824 - val_mean_squared_error: 0.2824\n",
      "Epoch 78/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: 1.0730 - mean_squared_error: 1.0730 - val_loss: 0.2712 - val_mean_squared_error: 0.2712\n",
      "Epoch 79/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0705 - mean_squared_error: 1.0705 - val_loss: 0.2746 - val_mean_squared_error: 0.2746\n",
      "Epoch 80/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.0711 - mean_squared_error: 1.0711 - val_loss: 0.2962 - val_mean_squared_error: 0.2962\n",
      "Epoch 81/100\n",
      "396/396 [==============================] - 0s 45us/step - loss: 1.0700 - mean_squared_error: 1.0700 - val_loss: 0.2845 - val_mean_squared_error: 0.2845\n",
      "Epoch 82/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.0688 - mean_squared_error: 1.0688 - val_loss: 0.2767 - val_mean_squared_error: 0.2767\n",
      "Epoch 83/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.0686 - mean_squared_error: 1.0686 - val_loss: 0.2711 - val_mean_squared_error: 0.2711\n",
      "Epoch 84/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: 1.0675 - mean_squared_error: 1.0675 - val_loss: 0.2658 - val_mean_squared_error: 0.2658\n",
      "Epoch 85/100\n",
      "396/396 [==============================] - 0s 49us/step - loss: 1.0685 - mean_squared_error: 1.0685 - val_loss: 0.2624 - val_mean_squared_error: 0.2624\n",
      "Epoch 86/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: 1.0656 - mean_squared_error: 1.0656 - val_loss: 0.2696 - val_mean_squared_error: 0.2696\n",
      "Epoch 87/100\n",
      "396/396 [==============================] - 0s 45us/step - loss: 1.0650 - mean_squared_error: 1.0650 - val_loss: 0.2678 - val_mean_squared_error: 0.2678\n",
      "Epoch 88/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: 1.0648 - mean_squared_error: 1.0648 - val_loss: 0.3027 - val_mean_squared_error: 0.3027\n",
      "Epoch 89/100\n",
      "396/396 [==============================] - 0s 44us/step - loss: 1.0670 - mean_squared_error: 1.0670 - val_loss: 0.2796 - val_mean_squared_error: 0.2796\n",
      "Epoch 90/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.0645 - mean_squared_error: 1.0645 - val_loss: 0.2724 - val_mean_squared_error: 0.2724\n",
      "Epoch 91/100\n",
      "396/396 [==============================] - 0s 47us/step - loss: 1.0653 - mean_squared_error: 1.0653 - val_loss: 0.2857 - val_mean_squared_error: 0.2857\n",
      "Epoch 92/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.0633 - mean_squared_error: 1.0633 - val_loss: 0.2741 - val_mean_squared_error: 0.2741\n",
      "Epoch 93/100\n",
      "396/396 [==============================] - 0s 46us/step - loss: 1.0637 - mean_squared_error: 1.0637 - val_loss: 0.2824 - val_mean_squared_error: 0.2824\n",
      "Epoch 94/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.0615 - mean_squared_error: 1.0615 - val_loss: 0.2891 - val_mean_squared_error: 0.2891\n",
      "Epoch 95/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0601 - mean_squared_error: 1.0601 - val_loss: 0.2763 - val_mean_squared_error: 0.2763\n",
      "Epoch 96/100\n",
      "396/396 [==============================] - 0s 44us/step - loss: 1.0614 - mean_squared_error: 1.0614 - val_loss: 0.3228 - val_mean_squared_error: 0.3228\n",
      "Epoch 97/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.0644 - mean_squared_error: 1.0644 - val_loss: 0.2816 - val_mean_squared_error: 0.2816\n",
      "Epoch 98/100\n",
      "396/396 [==============================] - 0s 45us/step - loss: 1.0601 - mean_squared_error: 1.0601 - val_loss: 0.2788 - val_mean_squared_error: 0.2788\n",
      "Epoch 99/100\n",
      "396/396 [==============================] - 0s 42us/step - loss: 1.0580 - mean_squared_error: 1.0580 - val_loss: 0.2817 - val_mean_squared_error: 0.2817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "396/396 [==============================] - 0s 45us/step - loss: 1.0581 - mean_squared_error: 1.0581 - val_loss: 0.2836 - val_mean_squared_error: 0.2836\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model did much, much better this time around!\n",
    "\n",
    "Run the cell below to get the model's predictions for both the training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 10 predictions from `pred_train`. Display those in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.13764419, -0.48095602, -0.30586046,  0.54349709, -0.08094749,\n",
       "       -0.04072087,  0.09825368,  0.10513391, -0.43215472, -0.06913403], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manually calculate the Mean Squared Error in the cell below.  \n",
    "\n",
    "As a refresher, here's the formula for calculating Mean Squared Error:\n",
    "\n",
    "<img src='mse_formula.gif'>\n",
    "\n",
    "Use `pred_train` and `Y_train` to calculate our training MSE in the cell below.  \n",
    "\n",
    "**_HINT:_** Use numpy to make short work of this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0533202252145233"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculate the MSE for our validation set in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28357290537863905"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_val = np.mean((pred_val-Y_val)**2)\n",
    "MSE_val "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Use weight initializers\n",
    "\n",
    "Another way to increase the performance of our models is to initialize our weights in clever ways.  We'll explore some of those options below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1  He initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and use a weight initializer.  We'll start with the **_He normalizer_**, which initializes the weight vector to have an average 0 and a variance of 2/n, with $n$ the number of features feeding into a layer.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Recreate the Neural Network that we created above.  This time, in the hidden layer, set the `kernel_initializer` to `\"he_normal\"`.\n",
    "* Compile and fit the model with the same hyperparameters as we used before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=12, kernel_initializer= \"he_normal\",\n",
    "                activation='relu'))\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val),verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "Run the cells below to get training and validation predictions are recalculate our MSE for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0937170669391523\n",
      "0.2951410757665797\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train) \n",
    "print(MSE_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initializer does not really help us to decrease the MSE. We know that initializers can be particularly helpful in deeper networks, and our network isn't very deep. What if we use the `Lecun` initializer with a `tanh` activation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2  Lecun initialization\n",
    "\n",
    "In the cell below, recreate the network again.  This time, set hidden layer's activation to `'tanh'`, and the `kernel_initializer` to `'lecun_normal'`.\n",
    "\n",
    "Then, fit and compile the model as did before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=12, \n",
    "                kernel_initializer= \"lecun_normal\", activation='tanh'))\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cells below to get the predictions and calculate the MSE for training and validation again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1090986602307809\n",
      "0.261894898816509\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train) \n",
    "print(MSE_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option we have is to play with the optimizers we choose for gradient descent during our back propagation step.  So far, we've only made use of basic `'sgd'`, or **_Stochastic Gradient Descent_**.  However, there are more advanced optimizers available to use will often converge to better minima, usually in a quicker fashion. \n",
    "\n",
    "In this lab, we'll try the two most popular methods: **_RMSprop_** and **_adam_**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 RMSprop\n",
    "\n",
    "In the cell below, recreate the original network that we built in this lab--no kernel intialization parameter, and the activation set to `'relu'`. \n",
    "\n",
    "This time, when you compile the model, set the `optimizer` parameter to `\"rmsprop\"`.  No changes to the `fit()` call are needed--keep those parameters the same.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"rmsprop\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to get predictions and compute the MSE again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0401446175465527\n",
      "0.29571405622862257\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train) \n",
    "print(MSE_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Adam\n",
    "\n",
    "Another popular optimizer is **_adam_**, which stands for `Adaptive Moment Estimation`. This is an optimzer that was created and open-sourced by a team at OpenAI, and is generally seen as the go-to choice for optimizers today. Adam combines the RMSprop algorithm with the concept of momentum, and is generally very effective at getting converging quickly.  \n",
    "\n",
    "In the cell below, create the same network that we did above, but this time, set the optimizer parameter to `'adam'`.  Leave all other parameters the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"Adam\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0780545941567379\n",
      "0.2531044843507429\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train) \n",
    "print(MSE_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Learning rate decay with momentum\n",
    "\n",
    "\n",
    "The final item that we'll get practice with in this lab is implementing a **_Learning Rate Decay_** strategy, along with **_Momentum_**.  We'll accomplish this by creating a `SGD` object and setting learning rate, decay, and momentum parameters at initialization.  In this way, we can then pass in the `SGD` object we've initialized to our specificataions during the compile step, rather than just a string representing an off-the-shelf `'SGD'`  optimizer.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create a `SGD` optimizer, which can be found in the `optimizers` module.  \n",
    "    * Set the `lr` parameter to  `0.03`.\n",
    "    * Set the `decay` parameter to `0.0001`\n",
    "    * Set the `momentum` parameter to `0.9`.\n",
    "* Recreate the same network we used during the previous example.  \n",
    "* Set the optimizer parameter during the compile step to the `sgd` object we created. \n",
    "* Fit the model with the same hyperparameters as we used before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.03, decay=0.0001, momentum=0.9)\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= sgd ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the cell below to calcluate the MSE for our final version of this model and see how a learning rate decay strategy affected the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9672024539323882\n",
      "0.3320629254936059\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train) \n",
    "print(MSE_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "\n",
    "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "\n",
    "https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "\n",
    "https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
